# Key Concepts

## Generative AI

Artificial Intelligence (AI) imitates human behavior by using machine learning to interact with the environment and execute tasks without explicit directions on what to output. Generative AI describes a category of capabilities within AI that create original content. People typically interact with generative AI that has been built into chat applications.

## Vector Database

A **vector database** is a database designed to store and manage vector embeddings, which are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, and tens of thousands of dimensions might be used to represent sophisticated data. A vector's position in this space represents its characteristics. Words, phrases, or entire documents, and images, audio, and other types of data can all be vectorized.A vector database that is **integrated** in a highly performant NoSQL or relational database provides additional capabilities. The **integrated vector database** in a NoSQL or relational database can store, index, and query embeddings alongside the corresponding original data. In this repo we help you understand via working example how vectors are generated when data is inserted into Azure AI Search from Azure Cosmos DB and is used for vector searches.

<img src='/media/01_RAGwithAISearch.png' width='680' height='300'>

Users then ask natural language questions using the web-based search bar user interface (User Prompts). These prompts are then changed search query with vectorized data and used to search in Azure AI Search. The results are then sent back to the user. All of the User Search History are stored in a Cosmos DB container along with the number of tokens consumed by each user prompt. In a production environment users would only be able to see their own sessions but this solution shows all sessions from all users.

## Generating & Searching vectors

Vectors are generated with [Azure OpenAI Embedding skill](https://learn.microsoft.com/en-us/azure/search/cognitive-search-skill-azure-openai-embedding). As soon as a new item is inserted into AI Search Index in desired field.Search query vectors are generated by the [Azure OpenAI Embedding skill](https://learn.microsoft.com/en-us/azure/search/vector-search-integrated-vectorization#using-integrated-vectorization-in-queries). The skill is configured to generate vectors on the `searchQuery` field in the index. The `searchQuery` field is a copy of the `name` field in the index. The `name` field is the field that is used to search on in the web-based front-end.The `searchQuery` field is used to generate vectors on the data in the index. The vectors are then stored in the `vectors` field in the index. The vectors are used to perform a vector search on the data in the index. The query response which includes the original source data is sent to Azure OpenAI Service to generate a completion which is then passed back to the user as a response.

<img src='/media/01_Generating&SearchingVectors.PNG' width='700' height='320'>

## Managing search context and history

Large language models such as ChatGPT do not keep any history of what prompts users sent it, or what completions it generated. It is up to the developer to do this. Keeping this history is necessary for two reasons. First, it allows users to ask follow-up questions without having to provide any context, while also allowing the user to have a conversation with the model. Second, the conversation history is useful when performing vector searche on data as it provides additional detail on what the user is looking for. As an example, if I asked our Intelligent Retail Agent what bikes it had available, it would return for me all of the bikes in stock. If I then asked, "what colors are available?", if I did not pass the first prompt and completion, the vector search would not know that the user was asking about bike colors and would likely not produce an accurate or meaningful response.

Another concept surfaced with conversation management centers around tokens. All calls to Azure OpenAI Service are limited by the number of tokens in a request and response. The number of tokens is dependant on the model being used. You see each model and its token limit on OpenAI's website on their [Models Overview page](https://platform.openai.com/docs/models/overview).

## Token management

One of the more challenging aspects to building RAG Pattern solutions is managing the tokens to stay within the maximum number of tokens that can be consumed in a single request (prompt) and response (completion). It's possible to build a prompt that consumes all of the tokens in the requests and leaves too few to produce a useful response. It's also possible to generate an exception from the Azure OpenAI Service if the request itself is over the token limit. You will need a way to measure token usage before sending the request. This is handled in the [OptimizePromptSize()](https://github.com/Azure/BuildYourOwnCopilot/blob/main/src/SemanticKernel/Chat/ChatBuilder.cs#L107) method in the ChatBuilder class. This method uses the SemanticKernel tokenizer, [GPT3Tokenizer](https://github.com/Azure/BuildYourOwnCopilot/blob/main/src/SemanticKernel/Chat/SemanticKernelTokenizer.cs). The utility takes text and generates an array of vectors. The number of elements in the array represent the number of tokens that will be consumed. It can also do the reverse and take an array of vectors and output text. In this method here we first generate the vectors on the data returned from our vector search, then if necessary, reduce the amount of data by calculating the number of vectors we can safely pass in our request to Azure OpenAI Service. Here is the flow of this method.

1. Measure the amount of tokens for the vector search results (rag data).
2. Measure the amount of tokens for the user prompt. This data is also used to capture what the user prompt tokens would be if processed without any additional data and stored in the user prompt message in the completions collection (more on that later).
3. Calculate if the amount of tokens used by the `search results` plus the `user prompt` plus the `conversation` + `completion` is greater than what the model will accept. If it is greater, then calculate how much to reduce the amount of data and `decode` the vector array we generated from the search results, back into text.
4. Finally, return the text from our search results as well as the number of tokens for the last User Prompt (this will get stored a bit later).
